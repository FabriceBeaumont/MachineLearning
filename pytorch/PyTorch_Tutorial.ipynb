{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a682a294-b26e-4e4d-b74b-4d9db1318307",
   "metadata": {},
   "source": [
    "# Deep Learning With PyTorch\n",
    "\n",
    "Using:\n",
    "* [Deep Learning With PyTorch - Full Course by Patrick Loeber](https://www.youtube.com/watch?v=c36lUUr864M&t=5s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2c5d3f-dd0e-4e88-a1f2-4ff27683c634",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "339da0b6-4e5a-47dc-bbc1-26286c4be028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba0ea3d0-22a2-4fe5-96b6-aceb7abd2bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27043aff-340f-444d-88f4-81fefbae3c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For NN Visualization\n",
    "import torch\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fa29c6-9dc7-4196-a378-f80fa0323454",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1878103b-f4f4-4d75-8425-62bc405932fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9003bf6d-6e6d-4f9d-ae4c-d212704869fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Basic tensor operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8e18ed3-2012-48ca-b42e-354b5d0e438a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.3853e-36])\n",
      "tensor([1.3844e-36, 0.0000e+00, 1.3843e-36, 0.0000e+00])\n",
      "tensor([[-1.0444e-24,  4.5560e-41],\n",
      "        [ 1.3852e-36,  0.0000e+00]])\n",
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.]]], dtype=torch.float16)\n",
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# An empty tensor (1d-tensor) equals a scalar value.\n",
    "examples = [\n",
    "    torch.empty(1),\n",
    "    torch.empty(4),\n",
    "    torch.empty(2, 2),\n",
    "    torch.ones(1, 2, 3, dtype=torch.float16),\n",
    "    torch.zeros(3, 5),\n",
    "]\n",
    "for x in examples:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ab12220-b277-410f-b389-b6b6a4695266",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [(torch.rand(2,2), torch.rand(2,2)) for _ in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "471f938e-24ae-49a9-82c9-84cac903b91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26 ms ± 17.9 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit for x, y in samples: x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ee26b2c-0511-4a85-9910-a2c4389bf3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.31 ms ± 7.69 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit for x, y in samples: torch.add(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3152c4cc-0662-4b72-a48b-92d9835135a3",
   "metadata": {},
   "source": [
    "Functions with a tailind underscore can be used to do inplace operations. E.g. `x.add_(y)` as `x += y` equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "706eb4ee-2a8c-418c-99b7-729b2bd6103c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "823 µs ± 205 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit for x, y in samples: x.add_(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289321bc-4335-4db0-9e64-01c8eb3e7a40",
   "metadata": {},
   "source": [
    "Summary of basic tensor functions:\n",
    "- `torch.rand()`\n",
    "- `torch.add()`\n",
    "- `torch.sub()`\n",
    "- `torch.mul()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a4b16c-dc3e-4768-a463-5aeef4b86975",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Resizing tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76f45f0d-48d2-45a3-b20b-1b3826368bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5717, 0.7741, 0.0849, 0.7694],\n",
      "        [0.7536, 0.3507, 0.3607, 0.7189],\n",
      "        [0.0287, 0.7415, 0.7122, 0.3119],\n",
      "        [0.7741, 0.5915, 0.1425, 0.4543]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(4,4)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3932eb66-030a-4851-a463-f8073631cdd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5717, 0.7741, 0.0849, 0.7694, 0.7536, 0.3507, 0.3607, 0.7189, 0.0287,\n",
      "        0.7415, 0.7122, 0.3119, 0.7741, 0.5915, 0.1425, 0.4543])\n",
      "tensor([0.5717, 0.7741, 0.0849, 0.7694, 0.7536, 0.3507, 0.3607, 0.7189, 0.0287,\n",
      "        0.7415, 0.7122, 0.3119, 0.7741, 0.5915, 0.1425, 0.4543])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all rows to one 1D-tensor.\n",
    "y0 = x.view(16)\n",
    "print(y0)\n",
    "# This is the same as using `flatten()`\n",
    "y1 = x.flatten()\n",
    "print(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8bdcafe5-c96d-4f5f-b610-d3f2234f8835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5717, 0.7741, 0.0849, 0.7694, 0.7536, 0.3507, 0.3607, 0.7189],\n",
      "        [0.0287, 0.7415, 0.7122, 0.3119, 0.7741, 0.5915, 0.1425, 0.4543]])\n",
      "torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "# One can use the dummy value '-1' to let pytorch determine the second dimension.\n",
    "y = x.view(-1, 8)\n",
    "print(y)\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9de51b8-b0e0-43e1-8659-d3ac0108a185",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Tensor conversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8ed506-3365-438c-9f3c-bc268f1c9019",
   "metadata": {},
   "source": [
    "Note that if the tenor is stored on the CPU, this will be a deep copy! Thus it is a reference and not a true copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0191932f-e8d3-476f-b433-f57859c147ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "<class 'torch.Tensor'>\n",
      "[1. 1. 1. 1. 1.]\n",
      "<class 'numpy.ndarray'>\n",
      "\n",
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)\n",
    "print(type(a))\n",
    "b = a.numpy()\n",
    "print(b)\n",
    "print(type(b))\n",
    "\n",
    "print()\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e6250cfe-043f-4ff0-b791-d3e32d790fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "<class 'numpy.ndarray'>\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "<class 'torch.Tensor'>\n",
      "\n",
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = np.ones(5)\n",
    "print(a)\n",
    "print(type(a))\n",
    "b = torch.from_numpy(a)\n",
    "print(b)\n",
    "print(type(b))\n",
    "\n",
    "print()\n",
    "a += 1\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223e51c1-a3fd-463c-b276-aafcb6262915",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Using a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2aa68cda-cad9-4f4c-8043-9ae2f15ce8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(True)\n",
    "    # Initialize the GPU as cuda device.\n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "    # Instantiate a variable on the GPU.\n",
    "    x = torch.ones(5, device=device)\n",
    "    \n",
    "    # Instantiate a variable on the CPU and move it to be stored on the GPU.\n",
    "    # Note that numpy can only handle CPU tensors!\n",
    "    y = torch.ones(5)\n",
    "    y = y.to(device)\n",
    "    y.add_(x)\n",
    "    y.to(\"cpu\")\n",
    "else:\n",
    "    print(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3d110d-6ef8-4dc0-87f5-9a47f1cacbd7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Gradients with Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bf5fa1-fa18-48dd-acd1-34637f7c3858",
   "metadata": {},
   "source": [
    "If needed later, one can specify that a gradient will need to be computed for a specific variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "665ae51f-d746-449a-bd4a-441c58f02438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4658, -0.9957, -0.1948, -1.4954,  0.1331], requires_grad=True)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(5, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5317c09e-01fc-4a64-9bd4-41121854e1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.4658, 1.0043, 1.8052, 0.5046, 2.1331], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that Pytorch tracs the required operations for the Backpropagation.\n",
    "y = x+2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "51b8b071-1af3-44bd-88dd-9a522a10b41d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12.1606,  2.0173,  6.5178,  0.5092,  9.1001], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = y*y*2\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4f0eb95b-d3bc-4ebf-be63-3f82d836b257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.0610, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_scalar = z.mean()\n",
    "z_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "75d36da9-14f2-4a73-adb4-8b2d35205366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([1.9727, 0.8034, 1.4442, 0.4036, 1.7065])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)\n",
    "# Trigger the backward pass by calling\n",
    "z_scalar.backward() # dz/dx\n",
    "# Now the gradient has been computed and can be used.\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1661790e-21b0-4fc2-bb75-67e0f14ff6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3000, 3.0000, 0.0030])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "z = x.add(2)\n",
    "z.mul_(3)\n",
    "\n",
    "# Now the output is not a scalar value but a vector. To trigger the backwards pass, another vector is needed.\n",
    "v = torch.tensor([.1, 1., .001], dtype=torch.float32)\n",
    "assert x.size() == v.size(), \"The vector must have the same dimension as the input dimension!\"\n",
    "\n",
    "z.backward(v) # dz/dx\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ade7332-a3e8-4488-8260-8fb148e5eab9",
   "metadata": {},
   "source": [
    "There are several options to turn of the `requires_grad`-flag:\n",
    "* `x.requires_grad_(False)`\n",
    "* `x.detach()`\n",
    "* `with torch.no_grad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2caf031a-c342-4255-9f99-0ffd5792c95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3268, 0.9561, 1.7758], requires_grad=True)\n",
      "tensor([0.3268, 0.9561, 1.7758])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "x.requires_grad_(False)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "da188e58-c7a4-4328-93d0-03cb31ad15a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.3683,  0.9145,  0.1720], requires_grad=True)\n",
      "tensor([-2.3683,  0.9145,  0.1720], requires_grad=True)\n",
      "tensor([-2.3683,  0.9145,  0.1720])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "y = x.detach()\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6d065ce3-2a29-4fc7-ab42-b2da5071dc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.0285,  0.6048, -2.3660], requires_grad=True)\n",
      "torch.no_grad - x\ttensor([ 2.0285,  0.6048, -2.3660], requires_grad=True)\n",
      "torch.no_grad - y\ttensor([ 4.0285,  2.6048, -0.3660])\n",
      "x\t\t\ttensor([ 2.0285,  0.6048, -2.3660], requires_grad=True)\n",
      "y\t\t\ttensor([ 4.0285,  2.6048, -0.3660], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "with torch.no_grad():\n",
    "    y = x.add(2)\n",
    "    print(f\"torch.no_grad - x\\t{x}\")\n",
    "    print(f\"torch.no_grad - y\\t{y}\")\n",
    "\n",
    "y = x.add(2)\n",
    "print(f\"x\\t\\t\\t{x}\")\n",
    "print(f\"y\\t\\t\\t{y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb5a4fc-763d-4598-bb6c-cf6c3360c2d5",
   "metadata": {},
   "source": [
    "## Tiny NN example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cef7c7-25da-4741-9198-1eeba4d11f2f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Gradients by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "364af106-e87c-4225-a20a-783fb9ed96b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "Epoch 1: w = 1.400, loss = 43.50000000\n",
      "Epoch 3: w = 2.184, loss = 3.71199965\n",
      "Epoch 5: w = 2.309, loss = 2.69342732\n",
      "Epoch 7: w = 2.330, loss = 2.66735172\n",
      "Epoch 9: w = 2.333, loss = 2.66668415\n",
      "Epoch 11: w = 2.333, loss = 2.66666722\n",
      "Epoch 13: w = 2.333, loss = 2.66666675\n",
      "Epoch 15: w = 2.333, loss = 2.66666675\n",
      "Epoch 17: w = 2.333, loss = 2.66666651\n",
      "Epoch 19: w = 2.333, loss = 2.66666651\n",
      "Prediction after training: f(5) = 11.667\n"
     ]
    }
   ],
   "source": [
    "# f = w * x\n",
    "\n",
    "# f = 2 * x\n",
    "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "Y = np.array([5, 6, 7, 8], dtype=np.float32)\n",
    "\n",
    "w = .0\n",
    "\n",
    "# Model prediction.\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# Loss function: MSE\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted - y)**2).mean()\n",
    "\n",
    "# Gradient.\n",
    "# MSE = 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N 2x (x*w - y)\n",
    "def gradient(x, y, y_predicted):\n",
    "    return np.dot(2*x, y_predicted - y).mean()\n",
    "\n",
    "print(f\"Prediction before training: f(5) = {forward(5):.3f}\")\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "nr_epochs = 20\n",
    "\n",
    "for epoch in range(nr_epochs):\n",
    "    # Prediction from the forward pass.\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    # Loss computation.\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # Gradient computation.\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "    \n",
    "    # Update the weights.\n",
    "    w -= learning_rate * dw\n",
    "    \n",
    "    if epoch % (nr_epochs//10) == 0:\n",
    "        print(f\"Epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}\")\n",
    "\n",
    "        \n",
    "print(f\"Prediction after training: f(5) = {forward(5):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfe1e7e-e07f-4c9e-a2ed-9293714c6d98",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Gradients by Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cad3939d-2596-49bf-a2ae-0bfb4b0283c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "Epoch 1: w = 0.350, loss = 43.50000000\n",
      "Epoch 11: w = 1.943, loss = 4.24934816\n",
      "Epoch 21: w = 2.256, loss = 2.72801042\n",
      "Epoch 31: w = 2.318, loss = 2.66904426\n",
      "Epoch 41: w = 2.330, loss = 2.66675878\n",
      "Epoch 51: w = 2.333, loss = 2.66667008\n",
      "Epoch 61: w = 2.333, loss = 2.66666675\n",
      "Epoch 71: w = 2.333, loss = 2.66666675\n",
      "Epoch 81: w = 2.333, loss = 2.66666651\n",
      "Epoch 91: w = 2.333, loss = 2.66666651\n",
      "Prediction after training: f(5) = 11.667\n"
     ]
    }
   ],
   "source": [
    "# f = w * x\n",
    "\n",
    "# f = 2 * x\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([5, 6, 7, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# Model prediction.\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# Loss function: MSE\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted - y)**2).mean()\n",
    "\n",
    "print(f\"Prediction before training: f(5) = {forward(5):.3f}\")\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "nr_epochs = 100\n",
    "\n",
    "for epoch in range(nr_epochs):\n",
    "    # Prediction from the forward pass.\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    # Loss computation.\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # Gradient computation. Backward pass.\n",
    "    l.backward() # dl/dw\n",
    "    \n",
    "    # Update the weights.\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        \n",
    "    # Reset the gradients.\n",
    "    w.grad.zero_()\n",
    "    \n",
    "    if epoch % (nr_epochs // 10) == 0:\n",
    "        print(f\"Epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}\")\n",
    "\n",
    "        \n",
    "print(f\"Prediction after training: f(5) = {forward(5):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a52e50-e6da-443c-a33a-dec1c34a89e1",
   "metadata": {},
   "source": [
    "### Using Torch.NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5506c9ed-0af1-4988-af3d-c8f41e9ce96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Design the model (input, output size, forward pass)\n",
    "# 2) Construct the loss and optimizer\n",
    "# 3) Training loop:\n",
    "# - forward pass: compute prediction\n",
    "# - backward pass: gradients\n",
    "# - update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a420f99-6929-417b-9491-794033a38d10",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "both arguments to matmul need to be at least 1D, but they are 0D and 2D",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m output_size \u001b[38;5;241m=\u001b[39m n_features\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(input_size, output_size)\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction before training: f(5) = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel(torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m5\u001b[39m))\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m     15\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: both arguments to matmul need to be at least 1D, but they are 0D and 2D"
     ]
    }
   ],
   "source": [
    "# y = w * x   ==>   w = 2\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[5], [6], [7], [8]], dtype=torch.float32)\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "print(f\"Prediction before training: f(5) = {model(torch.tensor(5)):.3f}\")\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "nr_epochs = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w], lr=learning_rate)\n",
    "\n",
    "for epoch in range(nr_epochs):\n",
    "    # Prediction from the forward pass.\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    # Loss computation.\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # Gradient computation. Backward pass.\n",
    "    l.backward() # dl/dw\n",
    "    \n",
    "    # Update the weights.\n",
    "    optimizer.step()\n",
    "        \n",
    "    # Reset the gradients.\n",
    "    optimizer.grad_zero_()\n",
    "    \n",
    "    if epoch % (nr_epochs // 10) == 0:\n",
    "        print(f\"Epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}\")\n",
    "\n",
    "        \n",
    "print(f\"Prediction after training: f(5) = {forward(5):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d2c86f-b74c-4d60-a1bb-7fdc40fd06e7",
   "metadata": {},
   "source": [
    "# NEXT CHAPTER ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6442857-0048-423d-aa13-aff2ea77d608",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_graph = draw_graph(resnet18(), input_size=(1,3,224,224), expand_nested=True)\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8a4082-21bc-4ab1-a10a-068fe9d4465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTINUE WITH: https://youtu.be/c36lUUr864M?si=Ed3ezOjp2TDZD6Ev&t=4899\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
